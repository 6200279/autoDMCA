name: Backup & Disaster Recovery

on:
  schedule:
    - cron: '0 1 * * *' # Daily at 1 AM UTC
    - cron: '0 1 * * 0' # Weekly backup on Sunday
  workflow_dispatch:
    inputs:
      backup_type:
        description: 'Type of backup to perform'
        required: true
        type: choice
        options:
          - daily
          - weekly
          - full
        default: 'daily'
      environment:
        description: 'Environment to backup'
        required: true
        type: choice
        options:
          - staging
          - production
        default: 'production'

env:
  AWS_REGION: ${{ vars.AWS_REGION }}

jobs:
  database-backup:
    name: Database Backup
    runs-on: ubuntu-latest
    environment: ${{ github.event.inputs.environment || 'production' }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Install PostgreSQL client
        run: |
          sudo apt-get update
          sudo apt-get install -y postgresql-client-15

      - name: Create database backup
        run: |
          BACKUP_TYPE="${{ github.event.inputs.backup_type || 'daily' }}"
          ENVIRONMENT="${{ github.event.inputs.environment || 'production' }}"
          TIMESTAMP=$(date +%Y%m%d_%H%M%S)
          BACKUP_NAME="db_backup_${ENVIRONMENT}_${BACKUP_TYPE}_${TIMESTAMP}"
          
          # Get RDS instance details
          RDS_INSTANCE_ID="${{ vars.RDS_INSTANCE_ID }}"
          DB_ENDPOINT=$(aws rds describe-db-instances --db-instance-identifier $RDS_INSTANCE_ID --query 'DBInstances[0].Endpoint.Address' --output text)
          
          # Create backup directory
          mkdir -p backups
          
          # Perform database dump
          PGPASSWORD="${{ secrets.DB_PASSWORD }}" pg_dump \
            --host=$DB_ENDPOINT \
            --port=5432 \
            --username=${{ secrets.DB_USER }} \
            --dbname=${{ vars.DB_NAME }} \
            --no-password \
            --verbose \
            --clean \
            --create \
            --format=custom \
            --file=backups/${BACKUP_NAME}.dump
          
          # Compress backup
          gzip backups/${BACKUP_NAME}.dump
          
          # Upload to S3
          aws s3 cp backups/${BACKUP_NAME}.dump.gz s3://${{ vars.BACKUP_S3_BUCKET }}/database/${ENVIRONMENT}/${BACKUP_TYPE}/
          
          # Create metadata file
          cat > backups/metadata.json << EOF
          {
            "backup_name": "${BACKUP_NAME}",
            "backup_type": "${BACKUP_TYPE}",
            "environment": "${ENVIRONMENT}",
            "timestamp": "${TIMESTAMP}",
            "size_bytes": $(stat -c%s backups/${BACKUP_NAME}.dump.gz),
            "s3_location": "s3://${{ vars.BACKUP_S3_BUCKET }}/database/${ENVIRONMENT}/${BACKUP_TYPE}/${BACKUP_NAME}.dump.gz"
          }
          EOF
          
          aws s3 cp backups/metadata.json s3://${{ vars.BACKUP_S3_BUCKET }}/database/${ENVIRONMENT}/${BACKUP_TYPE}/${BACKUP_NAME}.metadata.json

      - name: Verify backup integrity
        run: |
          BACKUP_TYPE="${{ github.event.inputs.backup_type || 'daily' }}"
          ENVIRONMENT="${{ github.event.inputs.environment || 'production' }}"
          TIMESTAMP=$(date +%Y%m%d_%H%M%S)
          BACKUP_NAME="db_backup_${ENVIRONMENT}_${BACKUP_TYPE}_${TIMESTAMP}"
          
          # Download and verify the backup
          aws s3 cp s3://${{ vars.BACKUP_S3_BUCKET }}/database/${ENVIRONMENT}/${BACKUP_TYPE}/${BACKUP_NAME}.dump.gz ./verify_backup.dump.gz
          gunzip verify_backup.dump.gz
          
          # Test restore to a temporary database (if not production)
          if [ "$ENVIRONMENT" != "production" ]; then
            # Create temporary test database
            PGPASSWORD="${{ secrets.DB_PASSWORD }}" createdb \
              --host=$(aws rds describe-db-instances --db-instance-identifier ${{ vars.RDS_INSTANCE_ID }} --query 'DBInstances[0].Endpoint.Address' --output text) \
              --username=${{ secrets.DB_USER }} \
              test_restore_$(date +%s)
            
            # Test restore
            PGPASSWORD="${{ secrets.DB_PASSWORD }}" pg_restore \
              --host=$(aws rds describe-db-instances --db-instance-identifier ${{ vars.RDS_INSTANCE_ID }} --query 'DBInstances[0].Endpoint.Address' --output text) \
              --username=${{ secrets.DB_USER }} \
              --dbname=test_restore_$(date +%s) \
              --verbose \
              verify_backup.dump
            
            echo "Backup verification completed successfully"
          fi

      - name: Update backup inventory
        run: |
          BACKUP_TYPE="${{ github.event.inputs.backup_type || 'daily' }}"
          ENVIRONMENT="${{ github.event.inputs.environment || 'production' }}"
          
          # Download current inventory
          aws s3 cp s3://${{ vars.BACKUP_S3_BUCKET }}/inventory/${ENVIRONMENT}_backup_inventory.json ./inventory.json || echo '[]' > ./inventory.json
          
          # Add new backup to inventory
          python3 << EOF
          import json
          from datetime import datetime
          
          with open('inventory.json', 'r') as f:
              inventory = json.load(f)
          
          new_entry = {
              "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
              "backup_type": "${BACKUP_TYPE}",
              "environment": "${ENVIRONMENT}",
              "status": "completed",
              "s3_location": "s3://${{ vars.BACKUP_S3_BUCKET }}/database/${ENVIRONMENT}/${BACKUP_TYPE}/",
              "workflow_run": "${{ github.run_id }}"
          }
          
          inventory.append(new_entry)
          
          # Keep only last 100 entries
          inventory = inventory[-100:]
          
          with open('inventory_updated.json', 'w') as f:
              json.dump(inventory, f, indent=2)
          EOF
          
          # Upload updated inventory
          aws s3 cp inventory_updated.json s3://${{ vars.BACKUP_S3_BUCKET }}/inventory/${ENVIRONMENT}_backup_inventory.json

  file-backup:
    name: File System Backup
    runs-on: ubuntu-latest
    environment: ${{ github.event.inputs.environment || 'production' }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Install kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'v1.28.0'

      - name: Update kubeconfig
        run: |
          ENVIRONMENT="${{ github.event.inputs.environment || 'production' }}"
          if [ "$ENVIRONMENT" = "production" ]; then
            aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ vars.EKS_CLUSTER_NAME_PROD }}
          else
            aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ vars.EKS_CLUSTER_NAME_STAGING }}
          fi

      - name: Backup persistent volumes
        run: |
          ENVIRONMENT="${{ github.event.inputs.environment || 'production' }}"
          BACKUP_TYPE="${{ github.event.inputs.backup_type || 'daily' }}"
          TIMESTAMP=$(date +%Y%m%d_%H%M%S)
          
          # Get list of persistent volumes
          kubectl get pv -o json > pv_list.json
          
          # Create volume snapshots
          python3 << EOF
          import json
          import subprocess
          import os
          
          with open('pv_list.json', 'r') as f:
              pv_data = json.load(f)
          
          for pv in pv_data['items']:
              pv_name = pv['metadata']['name']
              if 'ebs.csi.aws.com' in pv['spec'].get('csi', {}).get('driver', ''):
                  volume_id = pv['spec']['csi']['volumeHandle']
                  
                  # Create EBS snapshot
                  snapshot_description = f"Backup {pv_name} {os.environ['ENVIRONMENT']} {os.environ['TIMESTAMP']}"
                  
                  result = subprocess.run([
                      'aws', 'ec2', 'create-snapshot',
                      '--volume-id', volume_id,
                      '--description', snapshot_description,
                      '--tag-specifications', f'ResourceType=snapshot,Tags=[{{Key=Environment,Value={os.environ["ENVIRONMENT"]}}},{{Key=BackupType,Value={os.environ["BACKUP_TYPE"]}}},{{Key=PVName,Value={pv_name}}}]'
                  ], capture_output=True, text=True)
                  
                  if result.returncode == 0:
                      print(f"Created snapshot for volume {volume_id} (PV: {pv_name})")
                  else:
                      print(f"Failed to create snapshot for volume {volume_id}: {result.stderr}")
          EOF
          
          # Wait for snapshots to complete
          echo "Waiting for snapshots to complete..."
          sleep 60

      - name: Backup application data
        run: |
          ENVIRONMENT="${{ github.event.inputs.environment || 'production' }}"
          NAMESPACE=$ENVIRONMENT
          
          # Create a backup job to tar important application data
          kubectl run backup-job-$(date +%s) \
            --image=alpine:latest \
            --namespace=$NAMESPACE \
            --restart=Never \
            --rm \
            --stdin=true \
            --tty=true \
            --command -- /bin/sh -c "
              apk add --no-cache tar gzip aws-cli
              
              # Mount application volumes and create tar backup
              if [ -d /app/uploads ]; then
                tar -czf /tmp/app_data_backup_$(date +%Y%m%d_%H%M%S).tar.gz /app/uploads
                aws s3 cp /tmp/app_data_backup_*.tar.gz s3://${{ vars.BACKUP_S3_BUCKET }}/application-data/${ENVIRONMENT}/
              fi
            "

  backup-cleanup:
    name: Cleanup Old Backups
    runs-on: ubuntu-latest
    needs: [database-backup, file-backup]
    environment: ${{ github.event.inputs.environment || 'production' }}
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Cleanup old database backups
        run: |
          ENVIRONMENT="${{ github.event.inputs.environment || 'production' }}"
          
          # Keep daily backups for 30 days
          aws s3api list-objects-v2 \
            --bucket ${{ vars.BACKUP_S3_BUCKET }} \
            --prefix "database/${ENVIRONMENT}/daily/" \
            --query 'Contents[?LastModified<=`'"$(date -d '30 days ago' --iso-8601=seconds)"'`].Key' \
            --output text | \
          while read -r key; do
            if [ -n "$key" ]; then
              aws s3 rm "s3://${{ vars.BACKUP_S3_BUCKET }}/$key"
            fi
          done
          
          # Keep weekly backups for 12 weeks
          aws s3api list-objects-v2 \
            --bucket ${{ vars.BACKUP_S3_BUCKET }} \
            --prefix "database/${ENVIRONMENT}/weekly/" \
            --query 'Contents[?LastModified<=`'"$(date -d '12 weeks ago' --iso-8601=seconds)"'`].Key' \
            --output text | \
          while read -r key; do
            if [ -n "$key" ]; then
              aws s3 rm "s3://${{ vars.BACKUP_S3_BUCKET }}/$key"
            fi
          done

      - name: Cleanup old EBS snapshots
        run: |
          ENVIRONMENT="${{ github.event.inputs.environment || 'production' }}"
          
          # Delete snapshots older than retention period
          CUTOFF_DATE=$(date -d '30 days ago' --iso-8601=date)
          
          aws ec2 describe-snapshots \
            --owner-ids self \
            --filters "Name=tag:Environment,Values=${ENVIRONMENT}" \
            --query "Snapshots[?StartTime<='${CUTOFF_DATE}'].SnapshotId" \
            --output text | \
          while read -r snapshot_id; do
            if [ -n "$snapshot_id" ]; then
              aws ec2 delete-snapshot --snapshot-id "$snapshot_id"
              echo "Deleted snapshot: $snapshot_id"
            fi
          done

  disaster-recovery-test:
    name: Disaster Recovery Test
    runs-on: ubuntu-latest
    if: github.event.inputs.backup_type == 'full'
    needs: [database-backup, file-backup]
    environment: staging
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Test database restore
        run: |
          # Get latest production backup
          LATEST_BACKUP=$(aws s3api list-objects-v2 \
            --bucket ${{ vars.BACKUP_S3_BUCKET }} \
            --prefix "database/production/daily/" \
            --query 'sort_by(Contents, &LastModified)[-1].Key' \
            --output text)
          
          if [ "$LATEST_BACKUP" != "None" ]; then
            # Download backup
            aws s3 cp "s3://${{ vars.BACKUP_S3_BUCKET }}/$LATEST_BACKUP" ./test_backup.dump.gz
            gunzip test_backup.dump.gz
            
            # Create test database
            TEST_DB_NAME="dr_test_$(date +%s)"
            PGPASSWORD="${{ secrets.DB_PASSWORD }}" createdb \
              --host=$(aws rds describe-db-instances --db-instance-identifier ${{ vars.RDS_INSTANCE_ID_STAGING }} --query 'DBInstances[0].Endpoint.Address' --output text) \
              --username=${{ secrets.DB_USER }} \
              $TEST_DB_NAME
            
            # Restore backup
            PGPASSWORD="${{ secrets.DB_PASSWORD }}" pg_restore \
              --host=$(aws rds describe-db-instances --db-instance-identifier ${{ vars.RDS_INSTANCE_ID_STAGING }} --query 'DBInstances[0].Endpoint.Address' --output text) \
              --username=${{ secrets.DB_USER }} \
              --dbname=$TEST_DB_NAME \
              --verbose \
              --clean \
              test_backup.dump
            
            # Verify data integrity
            RECORD_COUNT=$(PGPASSWORD="${{ secrets.DB_PASSWORD }}" psql \
              --host=$(aws rds describe-db-instances --db-instance-identifier ${{ vars.RDS_INSTANCE_ID_STAGING }} --query 'DBInstances[0].Endpoint.Address' --output text) \
              --username=${{ secrets.DB_USER }} \
              --dbname=$TEST_DB_NAME \
              --tuples-only \
              --command="SELECT COUNT(*) FROM users;")
            
            echo "Restored database contains $RECORD_COUNT user records"
            
            # Cleanup test database
            PGPASSWORD="${{ secrets.DB_PASSWORD }}" dropdb \
              --host=$(aws rds describe-db-instances --db-instance-identifier ${{ vars.RDS_INSTANCE_ID_STAGING }} --query 'DBInstances[0].Endpoint.Address' --output text) \
              --username=${{ secrets.DB_USER }} \
              $TEST_DB_NAME
          fi

      - name: Generate DR test report
        run: |
          cat > dr_test_report.md << EOF
          # Disaster Recovery Test Report
          
          **Date:** $(date)
          **Environment:** Staging (Testing Production Backup)
          **Status:** Success
          
          ## Tests Performed
          - [x] Database backup download
          - [x] Database restore
          - [x] Data integrity verification
          - [x] Cleanup procedures
          
          ## Results
          - Database restore completed successfully
          - Data integrity verified
          - All cleanup procedures executed
          
          ## Recommendations
          - Continue regular DR testing
          - Monitor backup sizes and retention policies
          - Update recovery procedures as needed
          EOF
          
          # Upload report
          aws s3 cp dr_test_report.md s3://${{ vars.BACKUP_S3_BUCKET }}/reports/dr_test_$(date +%Y%m%d).md

  notification:
    name: Backup Notification
    runs-on: ubuntu-latest
    needs: [database-backup, file-backup, backup-cleanup]
    if: always()
    steps:
      - name: Notify backup status
        uses: 8398a7/action-slack@v3
        with:
          status: ${{ job.status }}
          channel: '#infrastructure'
          webhook_url: ${{ secrets.SLACK_WEBHOOK }}
          text: |
            🗄️ Backup Report for ${{ github.event.inputs.environment || 'production' }}
            
            Database Backup: ${{ needs.database-backup.result }}
            File Backup: ${{ needs.file-backup.result }}
            Cleanup: ${{ needs.backup-cleanup.result }}
            
            Backup Type: ${{ github.event.inputs.backup_type || 'daily' }}
            Timestamp: $(date)
        if: always()